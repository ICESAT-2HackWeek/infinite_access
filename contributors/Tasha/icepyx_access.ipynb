{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "047d376b-dd22-42c0-b876-dfc609f8a976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jupyter-ai 2.31.2 requires faiss-cpu!=1.8.0.post0,<2.0.0,>=1.8.0, which is not installed.\n",
      "dask-geopandas 0.4.3 requires dask[dataframe]>=2025.1.0, but you have dask 2024.12.1 which is incompatible.\n",
      "distributed 2025.3.0 requires dask==2025.3.0, but you have dask 2024.12.1 which is incompatible.\n",
      "pydantic-settings 2.8.1 requires python-dotenv>=0.21.0, but you have python-dotenv 0.20.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "## upgrade to icepyx dev breaks the query so don't use it yet, but it should be used for timing when it works\n",
    "# %pip install -q --upgrade git+https://github.com/icesat2py/icepyx.git@development\n",
    "# %pip install -q --upgrade icepyx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760b90f8-85dc-440a-bfdf-1c78d0cf5511",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import icepyx as ipx\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7fbb3b6-e0ef-42b4-95a5-87ba9759d4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly= [\n",
    "      {\n",
    "        \"lon\": 152.75574911647576,\n",
    "        \"lat\": -81.21829026763271\n",
    "      },\n",
    "      {\n",
    "        \"lon\": 157.3571761682183,\n",
    "        \"lat\": -80.71048087173712\n",
    "      },\n",
    "      {\n",
    "        \"lon\": 158.02766982432937,\n",
    "        \"lat\": -80.55426349923613\n",
    "      },\n",
    "      {\n",
    "        \"lon\": 157.09423747954733,\n",
    "        \"lat\": -80.39105506808406\n",
    "      },\n",
    "      {\n",
    "        \"lon\": 151.36217406651946,\n",
    "        \"lat\": -80.92240258329097\n",
    "      },\n",
    "      {\n",
    "        \"lon\": 152.75574911647576,\n",
    "        \"lat\": -81.21829026763271\n",
    "      }\n",
    "    ]\n",
    "\n",
    "\n",
    "# Extract lon and lat from the poly list\n",
    "lons = [point[\"lon\"] for point in poly]\n",
    "lats = [point[\"lat\"] for point in poly]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf09a941-8878-4a1e-93a5-be498dbb3821",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Build a query that points to the specific granule (no spatial filter needed)\n",
    "q = ipx.Query(\n",
    "    product='ATL06',\n",
    "    spatial_extent=[(lon, lat) for lon, lat in zip(lons, lats)],\n",
    "    date_range=['2019-12-02', '2019-12-02'],\n",
    "    tracks = ['1022'],\n",
    "    version='006',\n",
    ") # 10220511\n",
    "\n",
    "# Ask for S3 URLs (cloud=True) instead of HTTPS download links\n",
    "gran_ids, s3_urls = q.avail_granules(ids=True, cloud=True)\n",
    "\n",
    "# Create a cloud reader from the S3 URL and request specific vars\n",
    "reader = ipx.Read(s3_urls[0])\n",
    "reader.vars.append(\n",
    "    beam_list=['gt2l'],\n",
    "    var_list=['latitude','longitude','h_li','delta_time']\n",
    ")\n",
    "\n",
    "# Load into an xarray.Dataset (in memory, from S3)\n",
    "ds = reader.load()  # default return is an xarray Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866eac20-d8d2-4f57-8232-53408bc07601",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.plot.scatter(x=\"longitude\", y=\"latitude\", hue=\"h_li\", vmin=-100, vmax=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fa93c93-6fd1-483f-9af1-9fca3fe6787e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_37145/3058036565.py:30: UserWarning: this is an initial implementation of Parquet/Feather file support and associated metadata.  This is tracking version 0.1.0 of the metadata specification at https://github.com/geopandas/geo-arrow-spec\n",
      "\n",
      "This metadata specification does not yet make stability promises.  We do not yet recommend using this in a production setting unless you are able to rewrite your Parquet/Feather files.\n",
      "\n",
      "To further ignore this warning, you can do: \n",
      "import warnings; warnings.filterwarnings('ignore', message='.*initial implementation of Parquet.*')\n",
      "  all_gdf.to_parquet(\"data/all_segments.parquet\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "data_folder = \"data/\"\n",
    "gdf_list = []\n",
    "\n",
    "for filename in os.listdir(data_folder):\n",
    "    if filename.endswith(\".h5\"):\n",
    "        filepath = os.path.join(data_folder, filename)\n",
    "        with h5py.File(filepath, \"r\") as f:\n",
    "            # Example for ATL06 structure, adjust as needed\n",
    "            for beam in [k for k in f.keys() if k.startswith(\"gt\")]:\n",
    "                try:\n",
    "                    lat = f[f\"{beam}/land_ice_segments/latitude\"][:]\n",
    "                    lon = f[f\"{beam}/land_ice_segments/longitude\"][:]\n",
    "                    h_li = f[f\"{beam}/land_ice_segments/h_li\"][:]\n",
    "                    df = gpd.GeoDataFrame({\n",
    "                        \"h_li\": h_li,\n",
    "                        \"beam\": beam\n",
    "                    }, geometry=[Point(xy) for xy in zip(lon, lat)], crs=\"EPSG:4326\")\n",
    "                    gdf_list.append(df)\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "if gdf_list:\n",
    "    all_gdf = gpd.GeoDataFrame(pd.concat(gdf_list, ignore_index=True), crs=\"EPSG:4326\")\n",
    "    all_gdf.to_parquet(\"data/all_segments.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b50763f-8601-44b4-b37c-45931f5215c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
